{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfbd214",
   "metadata": {},
   "source": [
    "# **Heston PINN**\n",
    "---\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial V}{\\partial t} + rS \\frac{\\partial V}{\\partial S} + \\rho\\sigma v S \\frac{\\partial^2 V}{\\partial S \\partial v} + \\frac{1}{2}S^2 v \\frac{\\partial^2 V}{\\partial S^2} + \\frac{1}{2}\\sigma^2 v \\frac{\\partial^2 V}{\\partial v^2} + \\kappa(\\theta - v) \\frac{\\partial V}{\\partial v} - rV = 0\n",
    "$$\n",
    "<center>\n",
    "\n",
    "**Heston PDE**\n",
    "</center>\n",
    "\n",
    "Where:\n",
    "* $V$: Option price (a function of $S$, $v$, and $t$)\n",
    "* $t$: Time\n",
    "* $S$: Price of the underlying asset\n",
    "* $v$: Instantaneous variance of the underlying asset (volatility squared)\n",
    "* $r$: Risk-free interest rate\n",
    "* $\\rho$: Correlation between the Brownian motion of the asset price and its variance\n",
    "* $\\sigma$: Volatility of the variance (volatility of volatility)\n",
    "* $\\kappa$: Rate at which the variance $v$ reverts to its long-term mean $\\theta$\n",
    "* $\\theta$: Long-term mean variance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86388b47",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11007c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Common Parameters (from Black-Scholes)\n",
    "r = 0.05          # Risk-free rate\n",
    "sigma = 0.2       # Black-Scholes volatility (used for anchoring)\n",
    "K = 100.0         # Strike price\n",
    "T = 1.0           # Time to maturity (in years)\n",
    "\n",
    "# Heston specific Parameters\n",
    "# We set theta (long-term variance) and v_0 (initial variance)\n",
    "# to sigma^2 to make the models comparable.\n",
    "theta = sigma**2  # Long-term variance (0.04)\n",
    "v_0 = sigma**2    # Initial variance (0.04)\n",
    "kappa = 2.0       # Rate of mean reversion for variance\n",
    "sigma_v = 0.3     # Volatility of variance (\"vol of vol\")\n",
    "rho = -0.7        # Correlation between asset and variance\n",
    "\n",
    "# Domain and Grid Setup\n",
    "# Spatial domain for Stock Price (S)\n",
    "S_min = 0.0\n",
    "S_max = 250.0\n",
    "\n",
    "# Spatial domain for Variance (v)\n",
    "v_min = 0.0\n",
    "v_max = 1.0 # The max variance can be adjusted based on model behavior\n",
    "\n",
    "# Time domain\n",
    "t_min = 0.0\n",
    "t_max = T\n",
    "\n",
    "# Grid points\n",
    "N = 500\n",
    "\n",
    "# Create grids for S, t, and v\n",
    "# These represent the collocation points for training the PINN\n",
    "S_grid = torch.linspace(S_min, S_max, N).view(-1, 1).requires_grad_()\n",
    "t_grid = torch.linspace(t_min, t_max, N).view(-1, 1).requires_grad_()\n",
    "v_grid = torch.linspace(v_min, v_max, N).view(-1, 1).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6230ee",
   "metadata": {},
   "source": [
    "## **Define the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99adc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HestonPINN(nn.Module):\n",
    "    def __init__(self, input_dim=3, output_dim=1, hidden_dim=4, neurons_per_layer=64, activation_fn=nn.Tanh()):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, neurons_per_layer))\n",
    "        layers.append(activation_fn)\n",
    "\n",
    "        for _ in range(hidden_dim - 1):\n",
    "            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "            layers.append(activation_fn)\n",
    "\n",
    "        layers.append(nn.Linear(neurons_per_layer, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, S, v, t):\n",
    "        \"\"\"Concatenates inputs and performs a forward pass.\"\"\"\n",
    "        x = torch.cat([S, v, t], dim=1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9705c1a",
   "metadata": {},
   "source": [
    "## **Define the Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d086b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_loss(model, S, v, t):\n",
    "    # Calculates the residual of the Heston PDE\n",
    "\n",
    "    C = model(S, v, t)\n",
    "    \n",
    "    # First derivatives\n",
    "    C_t = torch.autograd.grad(C, t, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
    "    C_S = torch.autograd.grad(C, S, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
    "    C_v = torch.autograd.grad(C, v, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
    "    \n",
    "    # Second derivatives\n",
    "    C_SS = torch.autograd.grad(C_S, S, grad_outputs=torch.ones_like(C_S), create_graph=True)[0]\n",
    "    C_vv = torch.autograd.grad(C_v, v, grad_outputs=torch.ones_like(C_v), create_graph=True)[0]\n",
    "    C_Sv = torch.autograd.grad(C_S, v, grad_outputs=torch.ones_like(C_S), create_graph=True)[0]\n",
    "    \n",
    "    # Heston PDE residual\n",
    "    pde_residual = (\n",
    "        C_t\n",
    "        + r * S * C_S\n",
    "        + kappa * (theta - v) * C_v\n",
    "        + 0.5 * v * S**2 * C_SS\n",
    "        + 0.5 * sigma_v**2 * v * C_vv\n",
    "        + rho * sigma_v * v * S * C_Sv\n",
    "        - r * C\n",
    "    )\n",
    "    \n",
    "    return torch.mean(pde_residual**2)\n",
    "\n",
    "\n",
    "def boundary_loss(model, S_boundary, v_boundary, t_boundary):\n",
    "    #Calculates the loss at the spatial boundaries (S=0 and S=S_max)\n",
    "\n",
    "    # Loss at S=0 (option is worthless)\n",
    "    S_zero = torch.zeros_like(t_boundary).requires_grad_()\n",
    "    C_at_S_zero = model(S_zero, v_boundary, t_boundary)\n",
    "    loss_S_zero = torch.mean(C_at_S_zero**2)\n",
    "    \n",
    "    # Loss at S=S_max (option behaves like S - K*exp(-r(T-t)))\n",
    "    S_at_max = (torch.ones_like(t_boundary) * S_max).requires_grad_()\n",
    "    C_at_S_max_pred = model(S_at_max, v_boundary, t_boundary)\n",
    "    C_at_S_max_true = S_at_max - K * torch.exp(-r * (T - t_boundary))\n",
    "    loss_S_max = torch.mean((C_at_S_max_pred - C_at_S_max_true)**2)\n",
    "    \n",
    "    # Note: For v boundaries, enforcing the PDE is a common strategy.\n",
    "    # This is implicitly handled by including v_min and v_max points\n",
    "    # in the collocation points for the pde_loss.\n",
    "    \n",
    "    return loss_S_zero + loss_S_max\n",
    "\n",
    "\n",
    "def terminal_loss(model, S_terminal, v_terminal):\n",
    "    # Calculates the loss at the terminal condition (t=T), i.e., the payoff\n",
    "    \n",
    "    t_terminal = (torch.ones_like(S_terminal) * T).requires_grad_()\n",
    "    C_pred = model(S_terminal, v_terminal, t_terminal)\n",
    "    \n",
    "    # Payoff for a European Call option: max(S - K, 0)\n",
    "    C_true = torch.clamp(S_terminal - K, min=0)\n",
    "    \n",
    "    return torch.mean((C_pred - C_true)**2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e40661",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549b8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobys\\miniconda3\\envs\\star-pinn\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 2.2645e+03, PDE: 3.8660e-02, BC: 2.2149e+04, TC: 4.5281e+03\n",
      "Epoch [200/10000], Loss: 2.1877e+03, PDE: 8.9984e-02, BC: 2.1486e+04, TC: 4.2021e+03\n",
      "Epoch [300/10000], Loss: 2.1237e+03, PDE: 1.5639e-01, BC: 2.0990e+04, TC: 3.8230e+03\n",
      "Epoch [400/10000], Loss: 2.0924e+03, PDE: 2.1587e-01, BC: 2.0550e+04, TC: 3.9947e+03\n",
      "Epoch [500/10000], Loss: 2.0401e+03, PDE: 2.9568e-01, BC: 2.0030e+04, TC: 3.8922e+03\n",
      "Epoch [600/10000], Loss: 2.0087e+03, PDE: 3.9239e-01, BC: 1.9630e+04, TC: 3.9832e+03\n",
      "Epoch [700/10000], Loss: 1.9444e+03, PDE: 4.9423e-01, BC: 1.9177e+04, TC: 3.5109e+03\n",
      "Epoch [800/10000], Loss: 1.9092e+03, PDE: 5.9966e-01, BC: 1.8757e+04, TC: 3.5616e+03\n",
      "Epoch [900/10000], Loss: 1.8537e+03, PDE: 7.2912e-01, BC: 1.8363e+04, TC: 3.1517e+03\n",
      "Epoch [1000/10000], Loss: 1.8345e+03, PDE: 8.6027e-01, BC: 1.7944e+04, TC: 3.5142e+03\n",
      "Epoch [1100/10000], Loss: 1.7733e+03, PDE: 9.9876e-01, BC: 1.7560e+04, TC: 2.9712e+03\n",
      "Epoch [1200/10000], Loss: 1.7354e+03, PDE: 1.1568e+00, BC: 1.7183e+04, TC: 2.8797e+03\n",
      "Epoch [1300/10000], Loss: 1.6936e+03, PDE: 1.3110e+00, BC: 1.6787e+04, TC: 2.7422e+03\n",
      "Epoch [1400/10000], Loss: 1.6680e+03, PDE: 1.4837e+00, BC: 1.6437e+04, TC: 2.8433e+03\n",
      "Epoch [1500/10000], Loss: 1.6338e+03, PDE: 1.6650e+00, BC: 1.6051e+04, TC: 2.8347e+03\n",
      "Epoch [1600/10000], Loss: 1.5993e+03, PDE: 1.8555e+00, BC: 1.5702e+04, TC: 2.7547e+03\n",
      "Epoch [1700/10000], Loss: 1.5682e+03, PDE: 2.0792e+00, BC: 1.5357e+04, TC: 2.7248e+03\n",
      "Epoch [1800/10000], Loss: 1.5282e+03, PDE: 2.2645e+00, BC: 1.4984e+04, TC: 2.5767e+03\n",
      "Epoch [1900/10000], Loss: 1.4947e+03, PDE: 2.6156e+00, BC: 1.4633e+04, TC: 2.4869e+03\n",
      "Epoch [2000/10000], Loss: 1.4716e+03, PDE: 2.8469e+00, BC: 1.4278e+04, TC: 2.6361e+03\n",
      "Epoch [2100/10000], Loss: 1.4443e+03, PDE: 2.9490e+00, BC: 1.3959e+04, TC: 2.6592e+03\n",
      "Epoch [2200/10000], Loss: 1.4200e+03, PDE: 3.1916e+00, BC: 1.3602e+04, TC: 2.7861e+03\n",
      "Epoch [2300/10000], Loss: 1.3720e+03, PDE: 3.4449e+00, BC: 1.3301e+04, TC: 2.3336e+03\n",
      "Epoch [2400/10000], Loss: 1.3584e+03, PDE: 3.7056e+00, BC: 1.2990e+04, TC: 2.5870e+03\n",
      "Epoch [2500/10000], Loss: 1.3255e+03, PDE: 3.9748e+00, BC: 1.2665e+04, TC: 2.4769e+03\n",
      "Epoch [2600/10000], Loss: 1.3070e+03, PDE: 4.2518e+00, BC: 1.2368e+04, TC: 2.6032e+03\n",
      "Epoch [2700/10000], Loss: 1.2747e+03, PDE: 4.5363e+00, BC: 1.2090e+04, TC: 2.4145e+03\n",
      "Epoch [2800/10000], Loss: 1.2504e+03, PDE: 4.8289e+00, BC: 1.1764e+04, TC: 2.4748e+03\n",
      "Epoch [2900/10000], Loss: 1.2285e+03, PDE: 5.1285e+00, BC: 1.1478e+04, TC: 2.5079e+03\n",
      "Epoch [3000/10000], Loss: 1.2027e+03, PDE: 5.4354e+00, BC: 1.1227e+04, TC: 2.3985e+03\n",
      "Epoch [3100/10000], Loss: 1.1820e+03, PDE: 5.7462e+00, BC: 1.0912e+04, TC: 2.5051e+03\n",
      "Epoch [3200/10000], Loss: 1.1519e+03, PDE: 6.0688e+00, BC: 1.0644e+04, TC: 2.3368e+03\n",
      "Epoch [3300/10000], Loss: 1.1466e+03, PDE: 6.3977e+00, BC: 1.0408e+04, TC: 2.6058e+03\n",
      "Epoch [3400/10000], Loss: 1.1293e+03, PDE: 6.7282e+00, BC: 1.0138e+04, TC: 2.6958e+03\n",
      "Epoch [3500/10000], Loss: 1.0954e+03, PDE: 7.0689e+00, BC: 9.8513e+03, TC: 2.4827e+03\n",
      "Epoch [3600/10000], Loss: 1.0817e+03, PDE: 7.4146e+00, BC: 9.6172e+03, TC: 2.5761e+03\n",
      "Epoch [3700/10000], Loss: 1.0624e+03, PDE: 7.7940e+00, BC: 9.3464e+03, TC: 2.6175e+03\n",
      "Epoch [3800/10000], Loss: 1.0407e+03, PDE: 8.1210e+00, BC: 9.1313e+03, TC: 2.5210e+03\n",
      "Epoch [3900/10000], Loss: 1.0282e+03, PDE: 8.4841e+00, BC: 8.8728e+03, TC: 2.6790e+03\n",
      "Epoch [4000/10000], Loss: 1.0105e+03, PDE: 8.8486e+00, BC: 8.6569e+03, TC: 2.6563e+03\n",
      "Epoch [4100/10000], Loss: 9.9671e+02, PDE: 9.2276e+00, BC: 8.4349e+03, TC: 2.7183e+03\n",
      "Epoch [4200/10000], Loss: 9.7851e+02, PDE: 9.5970e+00, BC: 8.2085e+03, TC: 2.7031e+03\n",
      "Epoch [4300/10000], Loss: 9.7015e+02, PDE: 1.0069e+01, BC: 7.9671e+03, TC: 2.8932e+03\n",
      "Epoch [4400/10000], Loss: 9.4851e+02, PDE: 1.0360e+01, BC: 7.7546e+03, TC: 2.7979e+03\n",
      "Epoch [4500/10000], Loss: 9.3978e+02, PDE: 1.0939e+01, BC: 7.5753e+03, TC: 2.8468e+03\n",
      "Epoch [4600/10000], Loss: 9.2542e+02, PDE: 1.1175e+01, BC: 7.3450e+03, TC: 2.9401e+03\n",
      "Epoch [4700/10000], Loss: 9.1373e+02, PDE: 1.1634e+01, BC: 7.1606e+03, TC: 2.9614e+03\n",
      "Epoch [4800/10000], Loss: 9.0420e+02, PDE: 1.2309e+01, BC: 6.9568e+03, TC: 3.0214e+03\n",
      "Epoch [4900/10000], Loss: 8.8966e+02, PDE: 1.2325e+01, BC: 6.7899e+03, TC: 3.0349e+03\n",
      "Epoch [5000/10000], Loss: 8.8061e+02, PDE: 1.2722e+01, BC: 6.5796e+03, TC: 3.1680e+03\n",
      "Epoch [5100/10000], Loss: 8.7127e+02, PDE: 1.3189e+01, BC: 6.3974e+03, TC: 3.2305e+03\n",
      "Epoch [5200/10000], Loss: 8.5956e+02, PDE: 1.3569e+01, BC: 6.2268e+03, TC: 3.2404e+03\n",
      "Epoch [5300/10000], Loss: 8.5702e+02, PDE: 1.3981e+01, BC: 6.0540e+03, TC: 3.4319e+03\n",
      "Epoch [5400/10000], Loss: 8.4335e+02, PDE: 1.4340e+01, BC: 5.8881e+03, TC: 3.3978e+03\n",
      "Epoch [5500/10000], Loss: 8.4374e+02, PDE: 1.4763e+01, BC: 5.7345e+03, TC: 3.6104e+03\n",
      "Epoch [5600/10000], Loss: 8.3106e+02, PDE: 1.5130e+01, BC: 5.5706e+03, TC: 3.5910e+03\n",
      "Epoch [5700/10000], Loss: 8.2372e+02, PDE: 1.5567e+01, BC: 5.4351e+03, TC: 3.6132e+03\n",
      "Epoch [5800/10000], Loss: 8.1360e+02, PDE: 1.5926e+01, BC: 5.2819e+03, TC: 3.6267e+03\n",
      "Epoch [5900/10000], Loss: 8.0763e+02, PDE: 1.6331e+01, BC: 5.1319e+03, TC: 3.7091e+03\n",
      "Epoch [6000/10000], Loss: 7.9900e+02, PDE: 1.6729e+01, BC: 5.0002e+03, TC: 3.7054e+03\n",
      "Epoch [6100/10000], Loss: 8.0757e+02, PDE: 1.7091e+01, BC: 4.8718e+03, TC: 4.0467e+03\n",
      "Epoch [6200/10000], Loss: 7.9429e+02, PDE: 1.7478e+01, BC: 4.7413e+03, TC: 3.9501e+03\n",
      "Epoch [6300/10000], Loss: 8.0109e+02, PDE: 1.7911e+01, BC: 4.6118e+03, TC: 4.2449e+03\n",
      "Epoch [6400/10000], Loss: 7.8974e+02, PDE: 1.8320e+01, BC: 4.4957e+03, TC: 4.1562e+03\n",
      "Epoch [6500/10000], Loss: 7.8159e+02, PDE: 1.8579e+01, BC: 4.3811e+03, TC: 4.1563e+03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[0;32m     76\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 77\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Update learning rate scheduler\u001b[39;00m\n\u001b[0;32m     80\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(total_loss)\n",
      "File \u001b[1;32mc:\\Users\\tobys\\miniconda3\\envs\\star-pinn\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tobys\\miniconda3\\envs\\star-pinn\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tobys\\miniconda3\\envs\\star-pinn\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\tobys\\miniconda3\\envs\\star-pinn\\lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\tobys\\miniconda3\\envs\\star-pinn\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tobys\\miniconda3\\envs\\star-pinn\\lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobys\\miniconda3\\envs\\star-pinn\\lib\\site-packages\\torch\\optim\\adam.py:366\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m    364\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[0;32m    367\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim       = 3        # Fixed num of inputs (S, v, t)\n",
    "output_dim      = 1        # Fixed num of outputs (The price)\n",
    "hidden_dim      = 2        # Num hidden layers\n",
    "num_neurons     = 256      # Number of neurons per layer\n",
    "num_epochs      = 10000    # Training epochs\n",
    "learning_rate   = 0.0024   # Optimizer learning rate\n",
    "\n",
    "# Number of points to sample for each loss component\n",
    "N_pde = 2500\n",
    "N_boundary = 500\n",
    "N_terminal = 500\n",
    "\n",
    "# Loss weights (can be tuned)\n",
    "pde_weight = 9.188\n",
    "boundary_weight = 0.092\n",
    "terminal_weight = 0.05\n",
    "\n",
    "# Model, Optimizer, Scheduler\n",
    "model = HestonPINN(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    neurons_per_layer=num_neurons,\n",
    "    activation_fn=nn.Tanh()\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-7)\n",
    "\n",
    "# Training Loop\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Loss History Tracking\n",
    "loss_history = {\n",
    "    'total': [],\n",
    "    'pde': [],\n",
    "    'boundary': [],\n",
    "    'terminal': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1. PDE Loss (Interior Points)\n",
    "    # Sample random points in the (S, v, t) domain\n",
    "    S_pde = torch.rand(N_pde, 1) * (S_max - S_min) + S_min\n",
    "    v_pde = torch.rand(N_pde, 1) * (v_max - v_min) + v_min\n",
    "    t_pde = torch.rand(N_pde, 1) * (t_max - t_min) + t_min\n",
    "    S_pde.requires_grad = True\n",
    "    v_pde.requires_grad = True\n",
    "    t_pde.requires_grad = True\n",
    "    loss_pde = pde_loss(model, S_pde, v_pde, t_pde)\n",
    "\n",
    "    # 2. Boundary Loss (S=0 and S=S_max)\n",
    "    # Sample points on the v and t dimensions for the boundaries\n",
    "    v_bc = torch.rand(N_boundary, 1) * (v_max - v_min) + v_min\n",
    "    t_bc = torch.rand(N_boundary, 1) * (t_max - t_min) + t_min\n",
    "    v_bc.requires_grad = True\n",
    "    t_bc.requires_grad = True\n",
    "    loss_bc = boundary_loss(model, None, v_bc, t_bc) # S is handled inside the function\n",
    "\n",
    "    # 3. Terminal Loss (t=T)\n",
    "    # Sample points on the S and v dimensions for the terminal condition\n",
    "    S_tc = torch.rand(N_terminal, 1) * (S_max - S_min) + S_min\n",
    "    v_tc = torch.rand(N_terminal, 1) * (v_max - v_min) + v_min\n",
    "    S_tc.requires_grad = True\n",
    "    v_tc.requires_grad = True\n",
    "    loss_tc = terminal_loss(model, S_tc, v_tc)\n",
    "    \n",
    "    # Combine losses\n",
    "    total_loss = (pde_weight * loss_pde) + (boundary_weight * loss_bc) + (terminal_weight * loss_tc)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(total_loss)\n",
    "\n",
    "    # Record loss\n",
    "    loss_history['total'].append(total_loss.item())\n",
    "    loss_history['pde'].append(loss_pde.item())\n",
    "    loss_history['boundary'].append(loss_bc.item())\n",
    "    loss_history['terminal'].append(loss_tc.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4e}, \"\n",
    "              f\"PDE: {loss_pde.item():.4e}, BC: {loss_bc.item():.4e}, TC: {loss_tc.item():.4e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training finished in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107bd377",
   "metadata": {},
   "source": [
    "### **Plot Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd613d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "epochs_recorded = range(100, num_epochs + 1, 100)\n",
    "plt.plot(epochs_recorded, loss_history['total'], label='Total Loss')\n",
    "plt.plot(epochs_recorded, loss_history['pde'], label='PDE Loss', linestyle='--')\n",
    "plt.plot(epochs_recorded, loss_history['boundary'], label='Boundary Loss', linestyle='--')\n",
    "plt.plot(epochs_recorded, loss_history['terminal'], label='Terminal Loss', linestyle='--')\n",
    "\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5281f6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "star-pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
