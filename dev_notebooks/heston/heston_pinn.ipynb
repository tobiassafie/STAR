{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfbd214",
   "metadata": {},
   "source": [
    "# **Heston PINN**\n",
    "---\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial V}{\\partial t} + rS \\frac{\\partial V}{\\partial S} + \\rho\\sigma v S \\frac{\\partial^2 V}{\\partial S \\partial v} + \\frac{1}{2}S^2 v \\frac{\\partial^2 V}{\\partial S^2} + \\frac{1}{2}\\sigma^2 v \\frac{\\partial^2 V}{\\partial v^2} + \\kappa(\\theta - v) \\frac{\\partial V}{\\partial v} - rV = 0\n",
    "$$\n",
    "<center>\n",
    "\n",
    "**Heston PDE**\n",
    "</center>\n",
    "\n",
    "Where:\n",
    "* $V$: Option price (a function of $S$, $v$, and $t$)\n",
    "* $t$: Time\n",
    "* $S$: Price of the underlying asset\n",
    "* $v$: Instantaneous variance of the underlying asset (volatility squared)\n",
    "* $r$: Risk-free interest rate\n",
    "* $\\rho$: Correlation between the Brownian motion of the asset price and its variance\n",
    "* $\\sigma$: Volatility of the variance (volatility of volatility)\n",
    "* $\\kappa$: Rate at which the variance $v$ reverts to its long-term mean $\\theta$\n",
    "* $\\theta$: Long-term mean variance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86388b47",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11007c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Common Parameters (from Black-Scholes)\n",
    "r = 0.05          # Risk-free rate\n",
    "sigma = 0.2       # Black-Scholes volatility (used for anchoring)\n",
    "K = 100.0         # Strike price\n",
    "T = 1.0           # Time to maturity (in years)\n",
    "\n",
    "# Heston specific Parameters\n",
    "# We set theta (long-term variance) and v_0 (initial variance)\n",
    "# to sigma^2 to make the models comparable.\n",
    "theta = sigma**2  # Long-term variance (0.04)\n",
    "v_0 = sigma**2    # Initial variance (0.04)\n",
    "kappa = 2.0       # Rate of mean reversion for variance\n",
    "sigma_v = 0.3     # Volatility of variance (\"vol of vol\")\n",
    "rho = -0.7        # Correlation between asset and variance\n",
    "\n",
    "# Domain and Grid Setup\n",
    "# Spatial domain for Stock Price (S)\n",
    "S_min = 0.0\n",
    "S_max = 250.0\n",
    "\n",
    "# Spatial domain for Variance (v)\n",
    "v_min = 0.0\n",
    "v_max = 1.0 # The max variance can be adjusted based on model behavior\n",
    "\n",
    "# Time domain\n",
    "t_min = 0.0\n",
    "t_max = T\n",
    "\n",
    "# Grid points\n",
    "N = 500\n",
    "\n",
    "# Create grids for S, t, and v\n",
    "# These represent the collocation points for training the PINN\n",
    "S_grid = torch.linspace(S_min, S_max, N).view(-1, 1).requires_grad_()\n",
    "t_grid = torch.linspace(t_min, t_max, N).view(-1, 1).requires_grad_()\n",
    "v_grid = torch.linspace(v_min, v_max, N).view(-1, 1).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6230ee",
   "metadata": {},
   "source": [
    "## **Define the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99adc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HestonPINN(nn.Module):\n",
    "    def __init__(self, input_dim=3, output_dim=1, hidden_dim=4, neurons_per_layer=64, activation_fn=nn.Tanh()):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, neurons_per_layer))\n",
    "        layers.append(activation_fn)\n",
    "\n",
    "        for _ in range(hidden_dim - 1):\n",
    "            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "            layers.append(activation_fn)\n",
    "\n",
    "        layers.append(nn.Linear(neurons_per_layer, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, S, v, t):\n",
    "        \"\"\"Concatenates inputs and performs a forward pass.\"\"\"\n",
    "        x = torch.cat([S, v, t], dim=1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9705c1a",
   "metadata": {},
   "source": [
    "## **Define the Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d086b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_loss(model, S, v, t):\n",
    "    # Calculates the residual of the Heston PDE\n",
    "\n",
    "    C = model(S, v, t)\n",
    "    \n",
    "    # First derivatives\n",
    "    C_t = torch.autograd.grad(C, t, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
    "    C_S = torch.autograd.grad(C, S, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
    "    C_v = torch.autograd.grad(C, v, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
    "    \n",
    "    # Second derivatives\n",
    "    C_SS = torch.autograd.grad(C_S, S, grad_outputs=torch.ones_like(C_S), create_graph=True)[0]\n",
    "    C_vv = torch.autograd.grad(C_v, v, grad_outputs=torch.ones_like(C_v), create_graph=True)[0]\n",
    "    C_Sv = torch.autograd.grad(C_S, v, grad_outputs=torch.ones_like(C_S), create_graph=True)[0]\n",
    "    \n",
    "    # Heston PDE residual\n",
    "    pde_residual = (\n",
    "        C_t\n",
    "        + r * S * C_S\n",
    "        + kappa * (theta - v) * C_v\n",
    "        + 0.5 * v * S**2 * C_SS\n",
    "        + 0.5 * sigma_v**2 * v * C_vv\n",
    "        + rho * sigma_v * v * S * C_Sv\n",
    "        - r * C\n",
    "    )\n",
    "    \n",
    "    return torch.mean(pde_residual**2)\n",
    "\n",
    "\n",
    "def boundary_loss(model, S_boundary, v_boundary, t_boundary):\n",
    "    #Calculates the loss at the spatial boundaries (S=0 and S=S_max)\n",
    "\n",
    "    # Loss at S=0 (option is worthless)\n",
    "    S_zero = torch.zeros_like(t_boundary).requires_grad_()\n",
    "    C_at_S_zero = model(S_zero, v_boundary, t_boundary)\n",
    "    loss_S_zero = torch.mean(C_at_S_zero**2)\n",
    "    \n",
    "    # Loss at S=S_max (option behaves like S - K*exp(-r(T-t)))\n",
    "    S_at_max = (torch.ones_like(t_boundary) * S_max).requires_grad_()\n",
    "    C_at_S_max_pred = model(S_at_max, v_boundary, t_boundary)\n",
    "    C_at_S_max_true = S_at_max - K * torch.exp(-r * (T - t_boundary))\n",
    "    loss_S_max = torch.mean((C_at_S_max_pred - C_at_S_max_true)**2)\n",
    "    \n",
    "    # Note: For v boundaries, enforcing the PDE is a common strategy.\n",
    "    # This is implicitly handled by including v_min and v_max points\n",
    "    # in the collocation points for the pde_loss.\n",
    "    \n",
    "    return loss_S_zero + loss_S_max\n",
    "\n",
    "\n",
    "def terminal_loss(model, S_terminal, v_terminal):\n",
    "    # Calculates the loss at the terminal condition (t=T), i.e., the payoff\n",
    "    \n",
    "    t_terminal = (torch.ones_like(S_terminal) * T).requires_grad_()\n",
    "    C_pred = model(S_terminal, v_terminal, t_terminal)\n",
    "    \n",
    "    # Payoff for a European Call option: max(S - K, 0)\n",
    "    C_true = torch.clamp(S_terminal - K, min=0)\n",
    "    \n",
    "    return torch.mean((C_pred - C_true)**2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e40661",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549b8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobys\\anaconda3\\envs\\star-pinn\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [100/10000], Loss: 2.1269e+03, PDE: 3.7584e-02, BC: 2.2134e+04, TC: 4.4576e+03\n",
      "Epoch [200/10000], Loss: 2.0581e+03, PDE: 9.2667e-02, BC: 2.1479e+04, TC: 4.1909e+03\n",
      "Epoch [300/10000], Loss: 1.9878e+03, PDE: 1.6042e-01, BC: 2.0929e+04, TC: 3.7104e+03\n",
      "Epoch [400/10000], Loss: 1.9691e+03, PDE: 2.4000e-01, BC: 2.0469e+04, TC: 4.1055e+03\n",
      "Epoch [500/10000], Loss: 1.9115e+03, PDE: 3.3112e-01, BC: 1.9993e+04, TC: 3.7461e+03\n",
      "Epoch [600/10000], Loss: 1.8645e+03, PDE: 4.3344e-01, BC: 1.9566e+04, TC: 3.5100e+03\n",
      "Epoch [700/10000], Loss: 1.8572e+03, PDE: 5.4613e-01, BC: 1.9178e+04, TC: 4.0001e+03\n",
      "Epoch [800/10000], Loss: 1.7801e+03, PDE: 6.6879e-01, BC: 1.8778e+04, TC: 3.1091e+03\n",
      "Epoch [900/10000], Loss: 1.7637e+03, PDE: 8.0098e-01, BC: 1.8418e+04, TC: 3.3639e+03\n",
      "Epoch [1000/10000], Loss: 1.7213e+03, PDE: 9.4220e-01, BC: 1.8064e+04, TC: 3.0835e+03\n",
      "Epoch [1100/10000], Loss: 1.7090e+03, PDE: 1.0920e+00, BC: 1.7781e+04, TC: 3.2808e+03\n",
      "Epoch [1200/10000], Loss: 1.6606e+03, PDE: 1.2495e+00, BC: 1.7466e+04, TC: 2.8094e+03\n",
      "Epoch [1300/10000], Loss: 1.6402e+03, PDE: 1.4146e+00, BC: 1.7145e+04, TC: 2.9061e+03\n",
      "Epoch [1400/10000], Loss: 1.6088e+03, PDE: 1.5867e+00, BC: 1.6853e+04, TC: 2.7290e+03\n",
      "Epoch [1500/10000], Loss: 1.5862e+03, PDE: 1.7652e+00, BC: 1.6558e+04, TC: 2.7337e+03\n",
      "Epoch [1600/10000], Loss: 1.5774e+03, PDE: 1.9499e+00, BC: 1.6292e+04, TC: 2.9613e+03\n",
      "Epoch [1700/10000], Loss: 1.5566e+03, PDE: 2.1399e+00, BC: 1.6055e+04, TC: 2.8990e+03\n",
      "Epoch [1800/10000], Loss: 1.5258e+03, PDE: 2.3350e+00, BC: 1.5781e+04, TC: 2.6974e+03\n",
      "Epoch [1900/10000], Loss: 1.5012e+03, PDE: 2.5348e+00, BC: 1.5586e+04, TC: 2.4831e+03\n",
      "Epoch [2000/10000], Loss: 1.4801e+03, PDE: 2.7384e+00, BC: 1.5348e+04, TC: 2.4115e+03\n",
      "Epoch [2100/10000], Loss: 1.4721e+03, PDE: 2.9453e+00, BC: 1.5143e+04, TC: 2.5443e+03\n",
      "Epoch [2200/10000], Loss: 1.4411e+03, PDE: 3.1554e+00, BC: 1.4923e+04, TC: 2.2420e+03\n",
      "Epoch [2300/10000], Loss: 1.4495e+03, PDE: 3.3691e+00, BC: 1.4725e+04, TC: 2.6880e+03\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim       = 3        # Fixed num of inputs (S, v, t)\n",
    "output_dim      = 1        # Fixed num of outputs (The price)\n",
    "hidden_dim      = 16       # Num hidden layers\n",
    "num_neurons     = 16       # Number of neurons per layer\n",
    "num_epochs      = 10000    # Training epochs\n",
    "learning_rate   = 0.001    # Optimizer learning rate\n",
    "\n",
    "# Number of points to sample for each loss component\n",
    "N_pde = 2500\n",
    "N_boundary = 500\n",
    "N_terminal = 500\n",
    "\n",
    "# Loss weights (can be tuned)\n",
    "pde_weight = 14.46\n",
    "boundary_weight = 0.086\n",
    "terminal_weight = 0.05\n",
    "\n",
    "# Model, Optimizer, Scheduler\n",
    "model = HestonPINN(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    neurons_per_layer=num_neurons,\n",
    "    activation_fn=nn.Tanh()\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=500, factor=0.5, verbose=True)\n",
    "\n",
    "# Training Loop\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Loss History Tracking\n",
    "loss_history = {\n",
    "    'total': [],\n",
    "    'pde': [],\n",
    "    'boundary': [],\n",
    "    'terminal': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1. PDE Loss (Interior Points)\n",
    "    # Sample random points in the (S, v, t) domain\n",
    "    S_pde = torch.rand(N_pde, 1) * (S_max - S_min) + S_min\n",
    "    v_pde = torch.rand(N_pde, 1) * (v_max - v_min) + v_min\n",
    "    t_pde = torch.rand(N_pde, 1) * (t_max - t_min) + t_min\n",
    "    S_pde.requires_grad = True\n",
    "    v_pde.requires_grad = True\n",
    "    t_pde.requires_grad = True\n",
    "    loss_pde = pde_loss(model, S_pde, v_pde, t_pde)\n",
    "\n",
    "    # 2. Boundary Loss (S=0 and S=S_max)\n",
    "    # Sample points on the v and t dimensions for the boundaries\n",
    "    v_bc = torch.rand(N_boundary, 1) * (v_max - v_min) + v_min\n",
    "    t_bc = torch.rand(N_boundary, 1) * (t_max - t_min) + t_min\n",
    "    v_bc.requires_grad = True\n",
    "    t_bc.requires_grad = True\n",
    "    loss_bc = boundary_loss(model, None, v_bc, t_bc) # S is handled inside the function\n",
    "\n",
    "    # 3. Terminal Loss (t=T)\n",
    "    # Sample points on the S and v dimensions for the terminal condition\n",
    "    S_tc = torch.rand(N_terminal, 1) * (S_max - S_min) + S_min\n",
    "    v_tc = torch.rand(N_terminal, 1) * (v_max - v_min) + v_min\n",
    "    S_tc.requires_grad = True\n",
    "    v_tc.requires_grad = True\n",
    "    loss_tc = terminal_loss(model, S_tc, v_tc)\n",
    "    \n",
    "    # Combine losses\n",
    "    total_loss = (pde_weight * loss_pde) + (boundary_weight * loss_bc) + (terminal_weight * loss_tc)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(total_loss)\n",
    "\n",
    "    # Record loss\n",
    "    loss_history['total'].append(total_loss.item())\n",
    "    loss_history['pde'].append(loss_pde.item())\n",
    "    loss_history['boundary'].append(loss_bc.item())\n",
    "    loss_history['terminal'].append(loss_tc.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4e}, \"\n",
    "              f\"PDE: {loss_pde.item():.4e}, BC: {loss_bc.item():.4e}, TC: {loss_tc.item():.4e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training finished in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107bd377",
   "metadata": {},
   "source": [
    "### **Plot Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd613d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "epochs_recorded = range(100, num_epochs + 1, 100)\n",
    "plt.plot(epochs_recorded, loss_history['total'], label='Total Loss')\n",
    "plt.plot(epochs_recorded, loss_history['pde'], label='PDE Loss', linestyle='--')\n",
    "plt.plot(epochs_recorded, loss_history['boundary'], label='Boundary Loss', linestyle='--')\n",
    "plt.plot(epochs_recorded, loss_history['terminal'], label='Terminal Loss', linestyle='--')\n",
    "\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5281f6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "star-pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
